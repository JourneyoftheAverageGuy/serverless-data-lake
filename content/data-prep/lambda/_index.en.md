---
title: Lambda & Step Functions
weight: 240
pre: "<b>2.4. </b>"
---

# 2.4. Setup an AWS Glue ETL pipeline

![Data Lake Architecture](/images/modules/lambda.png?width=50pc)

> {{%expand "üéØüéØüéØ Deploy into Production ELT Pipeline" %}}
In the previous sections, you explored Raw NYC Taxi Trips Dataset. You then developed and tested ETL code to transform the Raw NYC Taxi Trips Dataset into a new Dataset that is optimized for querying and reporting by Unicorn-Taxi's Business End-Users. Now, it's time to deploy your Code into a Production ETL Pipeline using Glue.
{{% /expand%}}

### 2.4.1. Schedule AWS Glue Crawlers

> {{%expand "üéØWe'll start by scheduling the Raw Dataset AWS Glue Crawler" %}}
In this section, we'll build a **Schedule-Driven** AWS Glue ETL pipeline that looks as follows. We work our way backwards from a daily data availability Service Level Agreement (SLA) goal.
Unicorn-Taxi's Business End-Users need their Datasets to be refreshed daily and available by that time.
{{% /expand%}}

1. Navigate to the [AWS Glue console. In the left menu, click **Crawlers**](https://ap-southeast-1.console.aws.amazon.com/glue/home?region=ap-southeast-1#catalog:tab=crawlers)
2. Click on `nyctaxi-raw-crawler` , then click **Action** >> **Edit crawler**
3. In the left list of steps, click on **Schedule**
4. For **Frequency**, select `Daily`
5. Select `07:00 UTC` (2:00PM GMT+7)
6. In the left list, click on **Review all steps**
7. Scroll down and click **Finish**

> üéØNext, we'll schedule the **Optimized** Dataset AWS Glue Crawler.

1. Navigate to the [AWS Glue console. In the left menu, click **Crawlers**](https://ap-southeast-1.console.aws.amazon.com/glue/home?region=ap-southeast-1#catalog:tab=crawlers)
2. Click on `nyctaxi-optimized-crawler`, then click **Action** >> **Edit crawler**
3. In the left list of steps, click on **Schedule**
4. For **Frequency** , select `Daily`
5. Select `08:00 UTC` (3:00PM GMT+7)
6. In the left list, click on **Review all steps**
7. Scroll down and click **Finish**


### 2.4.2. Create an AWS Glue Job

The next step in setting up our AWS Glue ETL pipeline is to create an AWS Glue Job.

1. Navigate to the [AWS Glue console. In the left menu, under **ETL** , click **Jobs**](https://ap-southeast-1.console.aws.amazon.com/glue/home?region=ap-southeast-1#etl:tab=jobs) >> Click **Add job**

2. In step **Job properties** ...

    a. For **Name**, enter `nyctaxi-create-optimized-dataset`
    
    b. For **IAM role**, select `AWSGlueServiceRole-nyctaxi-optimize-job`
    
    c. For **This job runs**, select `An existing script that you provide`
    
    d. For **S3 path where the script is stored**, copy-and-paste this S3 URL:
    ```
    s3://serverless-data-lake-XXX/scripts/nyctaxi_create_optimized_dataset_job.py
    ```

    Replace `serverless-data-lake-XXX` with the actual name of your Amazon S3 bucket.

    e. For **Temporary directory**, specify the following S3 URL:
    ```
    s3://serverless-data-lake-XXX/data/tmp
    ```

    f. Expand section **Advanced properties**
        
    üëâ For **Job bookmark**, select `Enable`

    üëâ For **Monitoring options** >> **Job metrics**, select `Enable`

    g. Expand section **Security configuration, script libraries, and job parameters (optional)**

    üëâ For **Max concurrent** DPUs per Job run , enter 4
        
    üëâ Find the section named **Job parameters**
    
    Under **Key**, enter `--your_bucket_name` (two dashes, then the word **your_bucket_name** )

    Under **Value**, enter your actual Amazon S3 bucket name `serverless-data-lake-XXX`

    h. Click **Next**

3. ~~In step **Connections** , click **Next**~~
4. ~~In step **Review** , click **Save job and edit script**~~
5. In step **Connections** , click **Save job and edit script**
6. Review the script. Notice AWS Glue **job setup and teardown code**.
7. Click **X** on the top right to return to AWS Glue console. You have successfully created an AWS Glue job.

You can run this AWS Glue Job any time. With 4 DPUs allocated, it takes **about 7 - 9 minutes** from a cold start to complete. The job creates and writes an optimized NYC Taxi (Yellow) dataset to the following Amazon S3 path in your own account:

s3://serverless-data-lake-XXX/data/**prod**/nyctaxi/**yellow_rpt**

You can download the [Job's script here]()

> {{%expand "‚úçÔ∏è TRY IT OUT LATER" %}}
You can have AWS Glue auto-generate an ETL script for you! You can then edit and customize the script to your needs.

To accomplish that, in **Step 2.c** of the previous exercise, for **This job runs,** try to select **A proposed script generated by AWS Glue** instead. Follow the steps to create a fully-functional ETL script.
{{% /expand%}}


### 2.4.3. Create an AWS Glue Trigger

Finally, we'll create a **Time-based** AWS Glue Trigger to trigger our job through the following steps.

1. Navigate to the AWS Glue console
2. In the left menu, under **ETL**, click **Triggers**, then click **Add trigger** button
3. In step **Trigger properties** ‚Äô...

    a. For **Name**, enter `nyctaxi-process-raw-dataset`

    b. For **Trigger type**, select `Schedule`
    
    c. For **Frequency**, select `Custom`
    
    d. For **Cron expression**, enter `15 07 ? * * *` (mind the spaces!)
    
    e. Click **Next**

4. In step **Jobs to start** ...

    a. Under **Job**, look for `nyctaxi-optimize-raw-dataset`. Click **Add** next to it.
    
    b. Leave **Job bookmark** set to `Enabled`
    
    c. Click **Next**

5. In step **Review all steps** , check **Enable trigger on creation**
6. Click **Finish**

{{% notice note %}} 
**Congratulations! You have successfully set up your AWS Glue ETL pipeline.**
The pipeline will run on schedule. You can check your AWS Glue console at the times you scheduled to ensure that your Crawlers and your AWS Glue Job have run.
{{% /notice %}}